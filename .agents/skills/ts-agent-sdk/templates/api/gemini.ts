/**
 * ts-agent-sdk Gemini AI Client
 *
 * Typed wrapper for Google Gemini API.
 *
 * Environment Variables:
 * - SDK_GEMINI_API_KEY: Your Gemini API key
 * - SDK_GEMINI_MODEL: Default model (default: gemini-2.5-flash)
 */

import { post } from './base';
import { loadAPIConfig, validateGeminiConfig } from './config';

const GEMINI_API_BASE = 'https://generativelanguage.googleapis.com/v1beta/models';

export interface GenerateOptions {
  model?: string;
  maxTokens?: number;
  temperature?: number;
  topP?: number;
  stopSequences?: string[];
}

export interface GeminiContent {
  role: 'user' | 'model';
  parts: { text: string }[];
}

interface GeminiResponse {
  candidates: {
    content: {
      parts: { text: string }[];
    };
    finishReason: string;
  }[];
  usageMetadata?: {
    promptTokenCount: number;
    candidatesTokenCount: number;
    totalTokenCount: number;
  };
}

/**
 * Gemini AI Client for text generation, summarization, and structured extraction.
 */
export class GeminiClient {
  private apiKey: string;
  private defaultModel: string;

  constructor() {
    const config = loadAPIConfig();
    this.apiKey = config.gemini.apiKey;
    this.defaultModel = config.gemini.defaultModel;
  }

  /**
   * Generate text from a prompt.
   */
  async generateText(prompt: string, options: GenerateOptions = {}): Promise<string> {
    this.ensureConfigured();

    const model = options.model || this.defaultModel;
    const url = `${GEMINI_API_BASE}/${model}:generateContent?key=${this.apiKey}`;

    const response = await post<GeminiResponse>(url, {
      contents: [{ role: 'user', parts: [{ text: prompt }] }],
      generationConfig: {
        maxOutputTokens: options.maxTokens,
        temperature: options.temperature,
        topP: options.topP,
        stopSequences: options.stopSequences,
      },
    });

    return response.candidates[0]?.content?.parts[0]?.text || '';
  }

  /**
   * Generate a summary of the provided content.
   */
  async summarize(content: string, options: GenerateOptions = {}): Promise<string> {
    const prompt = `Please provide a concise summary of the following content:\n\n${content}`;
    return this.generateText(prompt, options);
  }

  /**
   * Extract structured JSON from content using a schema description.
   * @param prompt - Instructions for extraction including the content
   * @param schemaDescription - Description of the expected JSON structure
   */
  async extractJson<T>(prompt: string, schemaDescription: string): Promise<T> {
    const fullPrompt = `${prompt}

Respond ONLY with valid JSON matching this structure:
${schemaDescription}

Do not include any markdown formatting, code blocks, or explanations. Return only the raw JSON.`;

    const response = await this.generateText(fullPrompt, { temperature: 0 });

    // Clean up response - remove markdown code blocks if present
    let cleaned = response.trim();
    if (cleaned.startsWith('```json')) {
      cleaned = cleaned.slice(7);
    } else if (cleaned.startsWith('```')) {
      cleaned = cleaned.slice(3);
    }
    if (cleaned.endsWith('```')) {
      cleaned = cleaned.slice(0, -3);
    }

    return JSON.parse(cleaned.trim()) as T;
  }

  /**
   * Multi-turn conversation.
   */
  async chat(messages: GeminiContent[], options: GenerateOptions = {}): Promise<string> {
    this.ensureConfigured();

    const model = options.model || this.defaultModel;
    const url = `${GEMINI_API_BASE}/${model}:generateContent?key=${this.apiKey}`;

    const response = await post<GeminiResponse>(url, {
      contents: messages,
      generationConfig: {
        maxOutputTokens: options.maxTokens,
        temperature: options.temperature,
        topP: options.topP,
        stopSequences: options.stopSequences,
      },
    });

    return response.candidates[0]?.content?.parts[0]?.text || '';
  }

  private ensureConfigured(): void {
    const config = loadAPIConfig();
    validateGeminiConfig(config);
    this.apiKey = config.gemini.apiKey;
  }
}

// Default singleton instance
export const gemini = new GeminiClient();
